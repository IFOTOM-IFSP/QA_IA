
# IFOTOM QA ‚Äî Passo a passo completo

Este pacote √© um **guia execut√°vel** para voc√™ montar o fluxo de QA por Vis√£o Computacional do IFOTOM,
desde os dados at√© o servidor de API. Siga as etapas na ordem.

> **Resumo r√°pido**: voc√™ vai baixar pequenos datasets p√∫blicos (3‚Äì4 GB), converter r√≥tulos para YOLO-Seg,
unificar *splits*, treinar no Colab (YOLO-Seg nano) e rodar um servidor FastAPI com os endpoints `/qa/framecheck` e `/qa/yolo`.
O app manda miniaturas/ROI e recebe `issues` + `overlays`/m√©tricas para decidir repetir ou prosseguir.

---

## 0) Pr√©-requisitos

- **Git + VS Code** (organiza√ß√£o do repo e edi√ß√£o de c√≥digo).
- **Python 3.10+** (local) para convers√µes e o servidor FastAPI.
- **Colab (ou Kaggle)** para treinar o YOLO-Seg *nano* e validar.
- (Opcional) **DVC** para versionar dados grandes.

Estrutura deste pacote:
```
ifotom-qa-starter/
  datasets/ifotom/yolo_seg/         # destino final (YOLO-Seg) ap√≥s convers√µes
  framecheck/                        # trainer multi-r√≥tulo (FrameCheck)
  scripts/                           # conversores e utilit√°rios
  api/fastapi_app/                   # servidor FastAPI (stub funcional)
  train_seg.yaml                     # config de treino YOLO-Seg (Ultralytics)
  datasets/ifotom/yolo_seg_cuvette.yaml   # YAML do dataset (nomes e splits)
  requirements.txt                   # deps para scripts/servidor
  README.md                          # este guia
```

---

## 1) Baixar os datasets p√∫blicos (m√≠nimo vi√°vel)

Voc√™ **n√£o** precisa baixar cole√ß√µes enormes. Sugerido:
- **Vector-LabPics V1 (~3 GB)** ‚Äî aprender *vidraria + l√≠quido* (pr√©-treino).
- **Bubbles (He√üenkemper, ~500‚Äì600 MB)** ‚Äî m√°scaras de bolhas.
- **BubbleID (Dryad, ~1 GB)** ‚Äî bolhas / inst√¢ncias para robustez.

> Baixe **apenas** os zips principais de cada dataset. Consulte as p√°ginas oficiais para os links.
> Salve tudo em `data/raw/` (crie a pasta na raiz deste projeto).

**Exemplo de estrutura ap√≥s baixar/extrair:**
```
data/
  raw/
    LabPicsV1/ ... (conte√∫do extra√≠do)
    bubbles_hzdr/ ... (conte√∫do extra√≠do)
    bubbleid/ ... (conte√∫do extra√≠do)
```

---

## 2) Converter r√≥tulos para **seu esquema YOLO-Seg**

Esquema de classes:
```
0: cuvette
1: liquid
2: bubble
3: glare
4: smudge
```

- **LabPics ‚Üí cuvette/liquid**: use `scripts/convert_labpics.py` apontando para as anota√ß√µes do LabPics.
  Ele mapeia `vessel`‚Üí`cuvette` e `liquid`‚Üí`liquid` (ajuste se o dataset usar outro nome).
- **Bubbles (He√üenkemper) e BubbleID ‚Üí bubble**: use `scripts/convert_bubbles.py` para vetorizar m√°scaras (contornos) em pol√≠gonos YOLO-Seg.
- (Opcional) **glare** e **smudge**: rotule **20‚Äì50** imagens do seu dom√≠nio (com qualquer anotador de pol√≠gonos) e salve direto no formato YOLO-Seg.

**Comandos (exemplos):**
```bash
# Ambiente Python para scripts
python -m venv .venv && source .venv/bin/activate  # (Linux/macOS) ou .venv\Scripts\activate (Windows)
pip install -r requirements.txt

# Converter LabPics (ajuste --labpics-dir com o caminho real)
python scripts/convert_labpics.py   --labpics-dir data/raw/LabPicsV1   --out-dir datasets/ifotom/yolo_seg   --split all

# Converter bolhas (ajuste --images-dir/--masks-dir conforme dataset baixado)
python scripts/convert_bubbles.py   --images-dir data/raw/bubbles_hzdr/images   --masks-dir data/raw/bubbles_hzdr/masks   --out-dir datasets/ifotom/yolo_seg
```

Ao final, voc√™ ter√° **imagens** e **labels .txt** no formato YOLO-Seg dentro de `datasets/ifotom/yolo_seg/`.

---

## 3) Unificar *splits* (train/val/test) de forma estratificada

Rode:
```bash
python scripts/split_stratified.py   --yolo-root datasets/ifotom/yolo_seg   --train 0.80 --val 0.10 --test 0.10   --seed 42
```

Isto embaralha por **origem** e cria os diret√≥rios `images/{train,val,test}` e `labels/{train,val,test}`.

---

## 4) Treinar o **YOLO-Seg** no Colab

### 4.1 Suba este diret√≥rio para o seu Drive (ou d√™ `git clone` dentro do Colab).

### 4.2 Notebook Colab (copie/cole)
```python
!pip install -q ultralytics==8.3.0 opencv-python pillow onnx onnxruntime
from ultralytics import YOLO

# Caminhos: ajuste se necess√°rio (se montou Drive, aponte para sua pasta)
data_yaml = "datasets/ifotom/yolo_seg_cuvette.yaml"

# 1) Treino (modelo nano)
model = YOLO("yolov8n-seg.pt")     # ou yolo11n-seg.pt se dispon√≠vel
model.train(data=data_yaml, epochs=200, imgsz=640, batch=16,
            project="runs/ifotom-seg", name="cuvette-qa-seg-nano")

# 2) Valida√ß√£o
model.val()

# 3) Exportar (ONNX/TFLite) ‚Äî √∫til para on-device no futuro
model.export(format="onnx", dynamic=True)
model.export(format="tflite")
```

> Se der *OOM*, reduza `batch` para 8/4 ou `imgsz` para 512. Se quiser acelerar o *convergir*, congele camadas do backbone nas primeiras 20‚Äì40 √©pocas.

---

## 5) Treinar o **FrameCheck** (classifica√ß√£o multi-r√≥tulo)

Monte `framecheck/train.csv` e `framecheck/val.csv` assim:
```
filename,bubble,tilt,low_fill,glare,smudge,no_cuvette
path/to/img_0001.jpg,1,0,1,0,0,0
path/to/img_0002.jpg,0,0,0,1,1,0
...
```

Rode o trainer:
```bash
python framecheck/trainer.py   --train-csv framecheck/train.csv   --val-csv framecheck/val.csv   --epochs 60 --img-size 224   --out-dir framecheck/runs/exp1
```

Ele salva um `.pt` leve (MobileNet/EfficientNet-lite). Depois √© s√≥ carregar no servidor (`/qa/framecheck`).

---

## 6) Servidor FastAPI (stub funcional)

Instalar deps (local):
```bash
pip install -r requirements.txt
```

Rodar servidor:
```bash
uvicorn api.fastapi_app.main:app --reload --port 8000
```

- `POST /qa/framecheck` ‚Äî recebe miniaturas/ROI e retorna `issues` com *scores* (no stub, valores simulados).
- `POST /qa/yolo` ‚Äî recebe imagem e retorna `bbox_cuvette`, `metrics` (fill/tilt/bolha/glare) e m√°scaras (simuladas no stub).
- **Substitua os stubs** carregando seus pesos: YOLO (Ultralytics) para `/qa/yolo` e o classificador `.pt` para `/qa/framecheck`.

Acesse `http://127.0.0.1:8000/docs` para a UI Swagger (OpenAPI).

---

## 7) Integra√ß√£o no App (resumo)

- **Ap√≥s burst**: envie 1‚Äì3 miniaturas/ROI para `/qa/framecheck`.
  - Se `action=="repeat"`, **repita a captura** (e mostre o Marcinho com a dica).
  - Se `proceed`, continua o fluxo normal.
- (Opcional) Para *explicabilidade*, chame `/qa/yolo` em 1 frame do burst para mostrar overlay (bbox/m√°scaras) e m√©tricas (fill%, tilt).

**Overlay**: o servidor j√° retorna `polygons`/`bbox` no pixel-space. Desenhe em cima da preview.

---

## 8) Roadmap r√°pido (ap√≥s MVP)

1. Afinar thresholds no servidor (sem atualizar o app).
2. Adicionar classe `bubble` no YOLO-Seg (se come√ßar s√≥ com cuvette+liquid).
3. Treinar `glare`/`smudge` com poucas anota√ß√µes do seu ambiente.
4. Medir impacto: redu√ß√£o de CV da absorb√¢ncia e de medidas descartadas.

---

## 9) Licen√ßas e cita√ß√µes

- Cheque os termos de uso dos datasets (LabPics, He√üenkemper, BubbleID) e **cite os autores** quando publicar/demonstrar.
- Este starter √© MIT (veja cabe√ßalhos dos arquivos).

Bom trabalho! üöÄ
# QA_IA
